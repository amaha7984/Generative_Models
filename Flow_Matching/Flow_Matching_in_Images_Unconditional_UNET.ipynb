{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e629cc2f-b72b-4a67-be96-8a832f859de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------------\n",
    "# üìÅ Custom Dataset Loader\n",
    "# -----------------------------------\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root)\n",
    "                            if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# -----------------------------------\n",
    "# üîß Transform & Load Images\n",
    "# -----------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "image_size = 256\n",
    "\n",
    "\n",
    "image_dir = '/aul/homes/amaha038/Mapsgeneration/TerraFlySat_and_MapDatatset/TerraFly_Full_Satellite_Dataset/Philadelphia_Washington_Newyork_Train'  # Upload folder to this path in Colab\n",
    "dataset = ImageFolderDataset(image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c918a3-3917-4a50-8c55-1922750bf3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/a/bear.cs.fiu.edu./disk/bear-b/users/amaha038/GenAI/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DModel\n",
    "import torch.nn.functional as F\n",
    "class UNetFlowModel(nn.Module):\n",
    "    def __init__(self, image_size=256):\n",
    "        super().__init__()\n",
    "        self.unet = UNet2DModel(\n",
    "            sample_size=image_size,\n",
    "            in_channels=3,           # No need to manually concatenate time\n",
    "            out_channels=3,\n",
    "            layers_per_block=2,\n",
    "            block_out_channels=(64, 128, 256, 512),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # t must be passed as a LongTensor for timestep\n",
    "        t = (t * 999).long()  # scale to [0, 999] as expected\n",
    "        return self.unet(x, timestep=t).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30f723a-2b16-442e-9ccb-37959457bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "model = UNetFlowModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df861fc-8ff2-4bc2-930c-0f79d4469bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]:   1%|‚ñä                                                                                                                   | 4/594 [01:01<2:26:51, 14.93s/it, batch_loss=0.912]"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# üèãÔ∏è Training Loop\n",
    "# -----------------------------------\n",
    "loader = dataloader\n",
    "num_epochs = 5\n",
    "save_interval = 1\n",
    "batch_size = 16\n",
    "image_size = 256\n",
    "epoch_loss = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for x1 in pbar:\n",
    "        x1 = x1.to(device)\n",
    "        x0 = torch.randn_like(x1)\n",
    "        t = torch.rand(x1.size(0), device=device)\n",
    "\n",
    "        xt = (1 - t[:, None, None, None]) * x0 + t[:, None, None, None] * x1\n",
    "        target = x1 - x0\n",
    "\n",
    "        pred = model(xt, t)\n",
    "        loss = F.mse_loss(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    losses.append(avg_epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183c8b3-c179-40b0-84c6-0c8d888030f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Sampling from the Trained Model\n",
    "# -----------------------------------\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Seting model to eval mode and no gradients\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "# Generating starting noise\n",
    "num_samples = 5\n",
    "xt = torch.randn(num_samples, 196608).to(device)\n",
    "steps = 1000\n",
    "\n",
    "# Flow sampling\n",
    "for i, t in enumerate(torch.linspace(0, 1, steps)):\n",
    "    t_vec = t.expand(xt.size(0)).to(device)\n",
    "    xt = xt + (1 / steps) * model(xt, t_vec)\n",
    "\n",
    "# Reshape and unnormalize\n",
    "samples = xt.view(-1, 3, 256, 256)\n",
    "samples = samples.clamp(-1, 1) * 0.5 + 0.5  # [0, 1]\n",
    "\n",
    "# Folder to save\n",
    "output_dir = \"/path/to/save/dir\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Saving each image separately\n",
    "for i, img in enumerate(samples):\n",
    "    save_image(img, os.path.join(output_dir, f\"image_{i:03d}.png\"))\n",
    "\n",
    "print(f\"‚úÖ Saved {len(samples)} images to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbec51-3a54-44d1-9f60-5be3fd9dd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show preview\n",
    "import numpy as np\n",
    "plt.imshow(np.transpose(vutils.make_grid(samples.cpu(), nrow=4), (1, 2, 0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32316c9-9348-459d-9c48-2f1a7788d362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GenAI)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
