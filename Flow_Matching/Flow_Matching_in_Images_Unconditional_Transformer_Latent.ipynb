{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629cc2f-b72b-4a67-be96-8a832f859de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
    "from diffusers.models import AutoencoderKL\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------------\n",
    "# üìÅ Custom Dataset Loader\n",
    "# -----------------------------------\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root)\n",
    "                            if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# -----------------------------------\n",
    "# üîß Transform & Load Images\n",
    "# -----------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "image_size = 256\n",
    "\n",
    "\n",
    "image_dir = '/path/to/dataset'  \n",
    "dataset = ImageFolderDataset(image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c575e5-c82d-4b4b-a839-c547ed5dd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(device)\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dabe7f-6529-4675-84e2-5dc0ee0a3f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulate(x, shift, scale):\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    def __init__(self, hidden_size, freq_size=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(freq_size, hidden_size), nn.SiLU(), nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(t, dim, max_period=10000):\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(-math.log(max_period) * torch.arange(half, dtype=torch.float32) / half).to(t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        emb = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, t):\n",
    "        t_freq = self.timestep_embedding(t, 256)\n",
    "        return self.mlp(t_freq)\n",
    "\n",
    "class DiTBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = Mlp(hidden_size, hidden_size * 4)\n",
    "        self.adaLN_mod = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        shift1, scale1, gate1, shift2, scale2, gate2 = self.adaLN_mod(c).chunk(6, dim=1)\n",
    "        x = x + gate1.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift1, scale1))\n",
    "        x = x + gate2.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift2, scale2))\n",
    "        return x\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, patch_size, out_channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels)\n",
    "        self.adaLN_mod = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        shift, scale = self.adaLN_mod(c).chunk(2, dim=1)\n",
    "        x = modulate(self.norm(x), shift, scale)\n",
    "        return self.linear(x)\n",
    "\n",
    "class TransformerFlowLatent(nn.Module):\n",
    "    def __init__(self, latent_size=32, patch_size=2, in_channels=4, hidden_size=768, depth=12, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.img_size = latent_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = in_channels\n",
    "\n",
    "        self.embed = PatchEmbed(latent_size, patch_size, in_channels, hidden_size)\n",
    "        self.t_embed = TimestepEmbedder(hidden_size)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.embed.num_patches, hidden_size), requires_grad=False)\n",
    "\n",
    "        self.blocks = nn.ModuleList([DiTBlock(hidden_size, num_heads) for _ in range(depth)])\n",
    "        self.final = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        p = self.patch_size\n",
    "        h = w = self.img_size // p\n",
    "        C = self.out_channels\n",
    "        x = x.view(B, h, w, p, p, C).permute(0, 5, 1, 3, 2, 4)\n",
    "        return x.reshape(B, C, h * p, w * p)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.embed(x) + self.pos_embed\n",
    "        t_emb = self.t_embed(t)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, t_emb)\n",
    "        x = self.final(x, t_emb)\n",
    "        return self.unpatchify(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f723a-2b16-442e-9ccb-37959457bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "model = TransformerFlowLatent().to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df861fc-8ff2-4bc2-930c-0f79d4469bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# üèãÔ∏è Training Loop\n",
    "# -----------------------------------\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "num_epochs = 400\n",
    "save_every_n_epochs = 10\n",
    "num_samples_to_generate = 6\n",
    "\n",
    "save_dir = \"/path/to/generated_latent_outputs\"\n",
    "weights_dir = \"/path/to/saved_weights\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, x1 in enumerate(pbar):\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latents1 = vae.encode(x1).latent_dist.sample() * 0.18215\n",
    "            latents0 = torch.randn_like(latents1)\n",
    "\n",
    "        t = torch.rand(latents1.size(0), device=device)\n",
    "        xt = (1 - t[:, None, None, None]) * latents0 + t[:, None, None, None] * latents1\n",
    "        target = latents1 - latents0\n",
    "\n",
    "        pred = model(xt, t)\n",
    "        loss = F.mse_loss(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    # Save best weights\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), os.path.join(weights_dir, \"best_model.pth\"))\n",
    "        print(f\"üíæ Best model saved at epoch {epoch+1} with loss {best_loss:.4f}\")\n",
    "\n",
    "    # Save grid of samples every N epochs\n",
    "    if (epoch + 1) % save_every_n_epochs == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples_to_generate, 4, 32, 32).to(device)\n",
    "            t_sample = torch.ones(z.size(0), device=device)  # generate at t=1\n",
    "            generated_latents = model(z, t_sample)\n",
    "            decoded_imgs = vae.decode(generated_latents / 0.18215).sample\n",
    "            decoded_imgs = decoded_imgs.clamp(-1, 1) * 0.5 + 0.5  # [0, 1] for saving\n",
    "            grid = make_grid(decoded_imgs, nrow=4)  # change layout as needed\n",
    "            save_image(grid, os.path.join(save_dir, f\"epoch_{epoch+1:03d}.png\"))\n",
    "            print(f\"üñºÔ∏è Saved image grid for epoch {epoch+1}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), os.path.join(weights_dir, \"final_model.pth\"))\n",
    "print(\"‚úÖ Final model weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb932f-f734-4ebd-8f77-997e84fc7cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_plot = \"/pathto/plots\"\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir_plot, \"training_loss_curve.png\"))\n",
    "plt.close()\n",
    "print(\"üìà Saved training loss curve to training_loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183c8b3-c179-40b0-84c6-0c8d888030f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Sampling from the Trained Model\n",
    "# -----------------------------------\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "model.eval().requires_grad_(False)\n",
    "vae.eval().requires_grad_(False)\n",
    "\n",
    "# -----------------------------------\n",
    "# ‚öôÔ∏è Flow Matching in Latent Space\n",
    "# -----------------------------------\n",
    "num_samples = 5\n",
    "latent_size = 32  # VAE latent resolution for 256x256 images\n",
    "xt = torch.randn(num_samples, 4, latent_size, latent_size).to(device)  # starting from latent noise\n",
    "steps = 100\n",
    "\n",
    "# Integrate through flow field\n",
    "for t in torch.linspace(0, 1, steps, device=xt.device):\n",
    "    t_vec = t.expand(num_samples)\n",
    "    xt = xt + (1 / steps) * model(xt, t_vec)\n",
    "\n",
    "# -----------------------------------\n",
    "# üîÑ Decode Latents into RGB Images\n",
    "# -----------------------------------\n",
    "with torch.no_grad():\n",
    "    decoded = vae.decode(xt / 0.18215).sample  # match SD's latent scale\n",
    "    decoded = decoded.clamp(-1, 1) * 0.5 + 0.5  # [-1,1] ‚Üí [0,1] for visualization\n",
    "\n",
    "# -----------------------------------\n",
    "# üíæ Save Output Images\n",
    "# -----------------------------------\n",
    "output_dir = \"/path/to/Generated_Images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, img in enumerate(decoded):\n",
    "    save_image(img, os.path.join(output_dir, f\"image_{i:03d}.png\"))\n",
    "\n",
    "print(f\"‚úÖ Saved {len(decoded)} images to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbec51-3a54-44d1-9f60-5be3fd9dd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show preview\n",
    "import numpy as np\n",
    "plt.imshow(np.transpose(vutils.make_grid(samples.cpu(), nrow=4), (1, 2, 0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32316c9-9348-459d-9c48-2f1a7788d362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GenAI)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
