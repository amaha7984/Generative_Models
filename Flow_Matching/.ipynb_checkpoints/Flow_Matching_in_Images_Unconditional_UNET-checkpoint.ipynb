{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629cc2f-b72b-4a67-be96-8a832f859de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------------\n",
    "# üìÅ Custom Dataset Loader\n",
    "# -----------------------------------\n",
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.image_paths = [os.path.join(root, fname) for fname in os.listdir(root)\n",
    "                            if fname.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# -----------------------------------\n",
    "# üîß Transform & Load Images\n",
    "# -----------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "image_dir = 'path/to/dataset'  # Upload folder to this path in Colab\n",
    "dataset = ImageFolderDataset(image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c918a3-3917-4a50-8c55-1922750bf3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(channels, channels)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.ff(x))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, channels_data=196608, layers=5, channels=1024, channels_t=512):\n",
    "        super().__init__()\n",
    "        self.channels_t = channels_t\n",
    "        self.in_projection = nn.Linear(channels_data, channels)\n",
    "        self.t_projection = nn.Linear(channels_t, channels)\n",
    "        self.blocks = nn.Sequential(*[Block(channels) for _ in range(layers)])\n",
    "        self.out_projection = nn.Linear(channels, channels_data)\n",
    "\n",
    "    def gen_t_embedding(self, t, max_positions=10000):\n",
    "        t = t * max_positions\n",
    "        half_dim = self.channels_t // 2\n",
    "        emb = math.log(max_positions) / (half_dim - 1)\n",
    "        emb = torch.arange(half_dim, device=t.device).float().mul(-emb).exp()\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=1)\n",
    "        if self.channels_t % 2 == 1:\n",
    "            emb = nn.functional.pad(emb, (0, 1), mode='constant')\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.in_projection(x)\n",
    "        t = self.gen_t_embedding(t)\n",
    "        t = self.t_projection(t)\n",
    "        x = x + t\n",
    "        x = self.blocks(x)\n",
    "        x = self.out_projection(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f723a-2b16-442e-9ccb-37959457bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "model = MLP().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df861fc-8ff2-4bc2-930c-0f79d4469bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# üèãÔ∏è Training Loop\n",
    "# -----------------------------------\n",
    "num_epochs = 5\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    for batch in loop:\n",
    "        x1 = batch.to(device)\n",
    "        x1 = x1.view(x1.size(0), -1)  # Flatten image\n",
    "        x0 = torch.randn_like(x1).to(device)\n",
    "        target = x1 - x0\n",
    "\n",
    "        t = torch.rand(x1.size(0), device=device)\n",
    "        xt = (1 - t[:, None]) * x0 + t[:, None] * x1\n",
    "\n",
    "        pred = model(xt, t)\n",
    "        loss = ((target - pred)**2).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183c8b3-c179-40b0-84c6-0c8d888030f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® Sampling from the Trained Model\n",
    "# -----------------------------------\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Seting model to eval mode and no gradients\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "# Generating starting noise\n",
    "num_samples = 5\n",
    "xt = torch.randn(num_samples, 196608).to(device)\n",
    "steps = 1000\n",
    "\n",
    "# Flow sampling\n",
    "for i, t in enumerate(torch.linspace(0, 1, steps)):\n",
    "    t_vec = t.expand(xt.size(0)).to(device)\n",
    "    xt = xt + (1 / steps) * model(xt, t_vec)\n",
    "\n",
    "# Reshape and unnormalize\n",
    "samples = xt.view(-1, 3, 256, 256)\n",
    "samples = samples.clamp(-1, 1) * 0.5 + 0.5  # [0, 1]\n",
    "\n",
    "# Folder to save\n",
    "output_dir = \"/path/to/save/dir\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Saving each image separately\n",
    "for i, img in enumerate(samples):\n",
    "    save_image(img, os.path.join(output_dir, f\"image_{i:03d}.png\"))\n",
    "\n",
    "print(f\"‚úÖ Saved {len(samples)} images to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbec51-3a54-44d1-9f60-5be3fd9dd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show preview\n",
    "import numpy as np\n",
    "plt.imshow(np.transpose(vutils.make_grid(samples.cpu(), nrow=4), (1, 2, 0)))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32316c9-9348-459d-9c48-2f1a7788d362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GenAI)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
